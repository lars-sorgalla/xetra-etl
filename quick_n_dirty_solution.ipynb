{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35c0482c",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e711d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import polars as pl\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51678703",
   "metadata": {},
   "source": [
    "Job Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1eb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day from where you want to start the report\n",
    "start_date = datetime.date(year=2022, month=1, day=28)\n",
    "\n",
    "# end date of the report\n",
    "#   -> write `datetime.now()` if you want today's date as end date\n",
    "#   -> for exactly one day of output, enter same date as in `start_date` parameter\n",
    "end_date = datetime.date(year=2022, month=1, day=31)\n",
    "\n",
    "# fully-qualified name of S3 source bucket for raw data\n",
    "bucket_src_data = \"xetra-1234\"\n",
    "\n",
    "# fully-qualified name of S3 target bucket for transformed files\n",
    "bucket_tgt_data = \"xetra-project-transformed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80bf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate s3 client\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01c08002",
   "metadata": {},
   "source": [
    "List S3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_objects(s3_client: boto3.client, start_date: datetime.date, end_date: datetime.date, bucket_src_data: str) -> list[str]:\n",
    "    # get all file keys for data retrieval\n",
    "    listing_response: list[dict] = s3_client.list_objects_v2(Bucket=bucket_src_data)\n",
    "\n",
    "    # extract one day from start date. Needed for computing 1d percent change to previous day\n",
    "    start_date_min_1 = start_date - datetime.timedelta(days=1)\n",
    "\n",
    "    # list all file keys from `start date - 1` until `end date`\n",
    "    file_keys: list[str] = [\n",
    "        elem[\"Key\"] for elem in listing_response[\"Contents\"] \n",
    "        if (elem[\"Key\"].split(\"/\")[0] >= start_date_min_1.isoformat())  # start date as string\n",
    "        and (elem[\"Key\"].split(\"/\")[0] <= end_date.isoformat())  # end date as string\n",
    "    ]\n",
    "\n",
    "    return file_keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9d9899a",
   "metadata": {},
   "source": [
    "Fetch S3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f9c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_multiple_s3_objects(s3_client: boto3.client, file_keys: list[str], bucket_src_data: str) -> list[io.StringIO]:\n",
    "    # list of all csv string contents, one elem is the content of one csv file\n",
    "    file_like_csvs = []\n",
    "\n",
    "    # get file content\n",
    "    for key in file_keys:\n",
    "        \n",
    "        # get http response\n",
    "        get_objects_response: dict = s3_client.get_object(Bucket=bucket_src_data, Key=key)\n",
    "        \n",
    "        # read bytes stream content\n",
    "        bytes_content = get_objects_response.get(\"Body\").read()\n",
    "        \n",
    "        # convert bytes stream to string\n",
    "        string_content: str = bytes_content.decode(\"UTF-8\")\n",
    "        \n",
    "        # convert file content to file-like object for polars df creation\n",
    "        # alternative would be to save CSV locally, leading to extra IO processing time\n",
    "        file_like_csv = io.StringIO(string_content)\n",
    "\n",
    "        # return to start of stream\n",
    "        file_like_csv.seek(0)\n",
    "\n",
    "        # add to list of all contents\n",
    "        file_like_csvs.append(file_like_csv)\n",
    "\n",
    "    return file_like_csvs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c65e75c7",
   "metadata": {},
   "source": [
    "Create Polars DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b20a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polars_df(file_like_csvs: io.StringIO) -> pl.DataFrame:\n",
    "    # list of created dfs \n",
    "    df_list = []\n",
    "\n",
    "    # read content of each file into separate df\n",
    "    for single_csv_content in file_like_csvs:\n",
    "        \n",
    "        df = pl.read_csv(single_csv_content, try_parse_dates=True)\n",
    "        \n",
    "        # some files have no content, those shall be skipped\n",
    "        if df.height > 0:\n",
    "            df_list.append(df)\n",
    "\n",
    "    # merge all dfs from each file into one df\n",
    "    df_raw: pl.DataFrame  = pl.concat(df_list)\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcb63da2",
   "metadata": {},
   "source": [
    "Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc57316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # only the columns needed for our final report\n",
    "    df_less_cols = df.select(['ISIN','SecurityDesc','Date','Time','StartPrice','MaxPrice','MinPrice','EndPrice','TradedVolume'])\n",
    "\n",
    "    # cast 'Time' field from 'str' to 'time' data type\n",
    "    df_convert_time_dtype = df_less_cols.with_columns(pl.col(\"Time\").str.strptime(pl.Time, \"%H:%M\"))\n",
    "\n",
    "    # get opening price for ISIN per day\n",
    "    df_open_per_isin_n_day = df_convert_time_dtype.with_columns(pl.col(\"StartPrice\").first().over([\"ISIN\", \"Date\"]).alias(\"opening_price_1d\"))\n",
    "\n",
    "    # get closing price for ISIN per day\n",
    "    df_close_per_isin_n_day = df_open_per_isin_n_day.with_columns(pl.col(\"EndPrice\").first().over([\"ISIN\", \"Date\"]).alias(\"closing_price_1d\"))\n",
    "\n",
    "    # remove redundant columns\n",
    "    df_dropped_cols = df_close_per_isin_n_day.drop([\"StartPrice\", \"EndPrice\", \"Time\"])\n",
    "\n",
    "    # aggregate the windowed df to one record per ISIN per day\n",
    "    df_agg_by_isin_n_day = (df_dropped_cols.groupby([\"ISIN\", \"SecurityDesc\", \"Date\"])\n",
    "                                          .agg([\n",
    "                                            pl.first(\"opening_price_1d\").alias(\"open\")\n",
    "                                            , pl.max(\"MaxPrice\").alias(\"high\")\n",
    "                                            , pl.min(\"MinPrice\").alias(\"low\")\n",
    "                                            , pl.first(\"closing_price_1d\").alias(\"close\")\n",
    "                                            , pl.sum(\"TradedVolume\").alias(\"volume\")\n",
    "                                          ])\n",
    "    )\n",
    "\n",
    "    # order by ISIN and date to prepare for percent calculation in the next step\n",
    "    df_sort_by_isin_n_day = df_agg_by_isin_n_day.sort([\"ISIN\", \"Date\"])\n",
    "\n",
    "    # add col for prev day close as a basis for daily pct change computation\n",
    "    df_close_prev_day = df_sort_by_isin_n_day.with_columns(\n",
    "        pl.col(\"close\")\n",
    "        .shift(1)\n",
    "        .over([\"ISIN\"])\n",
    "        .alias(\"close_previous_day\")\n",
    "    )\n",
    "\n",
    "    # add col daily percent change\n",
    "    col_pct_change_1d = ( pl.col(\"close\") - pl.col(\"close_previous_day\") ) / pl.col(\"close\") * 100\n",
    "    df_pct_change_1d = df_close_prev_day.with_columns(col_pct_change_1d.round(2).alias(\"pct_change_1d\"))\n",
    "\n",
    "    # drop helper column for pct change calculation\n",
    "    df_dropped_cols_2 = df_pct_change_1d.drop(\"close_previous_day\")\n",
    "\n",
    "    # drop previous day's rows for pct change calculation\n",
    "    df_remove_prev_day_rows = df_dropped_cols_2.filter(pl.col(\"Date\") >= start_date)\n",
    "\n",
    "    return df_remove_prev_day_rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dcaa39c",
   "metadata": {},
   "source": [
    "Write To S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3b3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(df: pl.DataFrame) -> str:\n",
    "    # create empty in-memory file object\n",
    "    buffered_bytes_file_obj = io.BytesIO()\n",
    "\n",
    "    # write df to in-memory parquet file object\n",
    "    df.write_parquet(file=buffered_bytes_file_obj)\n",
    "\n",
    "    # return to start of stream\n",
    "    buffered_bytes_file_obj.seek(0)\n",
    "\n",
    "    # generate Key as object name for file upload\n",
    "    if start_date.isoformat() == end_date.isoformat():\n",
    "        key = \"xetra_report_daily_\" + start_date.isoformat() + \".parquet\"\n",
    "    else:\n",
    "        key = \"xetra_report_multiple_days_\" + start_date.isoformat() + \"_to_\" + end_date.isoformat() + \".parquet\"\n",
    "\n",
    "    # upload to S3\n",
    "    s3_client.upload_fileobj(\n",
    "        Fileobj=buffered_bytes_file_obj\n",
    "        ,Bucket=bucket_tgt_data\n",
    "        ,Key=key\n",
    "    )\n",
    "\n",
    "    # close bytes stream object\n",
    "    buffered_bytes_file_obj.close()\n",
    "\n",
    "    return key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c68824c8",
   "metadata": {},
   "source": [
    "Check Uploaded File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7256640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_single_s3_object(s3_client: boto3.client, bucket_tgt_data: str, obj_key: str) -> pl.DataFrame:\n",
    "    parquet_obj = s3_client.get_object(Bucket=bucket_tgt_data, Key=obj_key).get(\"Body\").read()\n",
    "    buffered_data = io.BytesIO(parquet_obj)\n",
    "    df = pl.read_parquet(buffered_data)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f45d0034",
   "metadata": {},
   "source": [
    "Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46739731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (6_346, 9)\n",
      "┌──────────────┬───────────────┬────────────┬────────┬───┬────────┬────────┬────────┬──────────────┐\n",
      "│ ISIN         ┆ SecurityDesc  ┆ Date       ┆ open   ┆ … ┆ low    ┆ close  ┆ volume ┆ pct_change_1 │\n",
      "│ ---          ┆ ---           ┆ ---        ┆ ---    ┆   ┆ ---    ┆ ---    ┆ ---    ┆ d            │\n",
      "│ str          ┆ str           ┆ date       ┆ f64    ┆   ┆ f64    ┆ f64    ┆ i64    ┆ ---          │\n",
      "│              ┆               ┆            ┆        ┆   ┆        ┆        ┆        ┆ f64          │\n",
      "╞══════════════╪═══════════════╪════════════╪════════╪═══╪════════╪════════╪════════╪══════════════╡\n",
      "│ AT000000STR1 ┆ STRABAG SE    ┆ 2022-01-28 ┆ 38.05  ┆ … ┆ 37.0   ┆ 38.05  ┆ 456    ┆ 1.18         │\n",
      "│ AT000000STR1 ┆ STRABAG SE    ┆ 2022-01-31 ┆ 37.8   ┆ … ┆ 37.65  ┆ 37.8   ┆ 1492   ┆ -0.66        │\n",
      "│ AT00000FACC2 ┆ FACC AG       ┆ 2022-01-28 ┆ 7.66   ┆ … ┆ 7.52   ┆ 7.66   ┆ 610    ┆ 0.26         │\n",
      "│              ┆ INH.AKT.      ┆            ┆        ┆   ┆        ┆        ┆        ┆              │\n",
      "│ AT0000606306 ┆ RAIFFEISEN BK ┆ 2022-01-28 ┆ 25.02  ┆ … ┆ 24.66  ┆ 25.02  ┆ 213    ┆ 3.44         │\n",
      "│              ┆ INTL INH.     ┆            ┆        ┆   ┆        ┆        ┆        ┆              │\n",
      "│ …            ┆ …             ┆ …          ┆ …      ┆ … ┆ …      ┆ …      ┆ …      ┆ …            │\n",
      "│ XS2314660700 ┆ GPF PHY NICK  ┆ 2022-01-28 ┆ 20.328 ┆ … ┆ 20.054 ┆ 20.328 ┆ 58     ┆ 0.6          │\n",
      "│              ┆ ETC           ┆            ┆        ┆   ┆        ┆        ┆        ┆              │\n",
      "│ XS2314660700 ┆ GPF PHY NICK  ┆ 2022-01-31 ┆ 20.212 ┆ … ┆ 20.17  ┆ 20.212 ┆ 846    ┆ -0.57        │\n",
      "│              ┆ ETC           ┆            ┆        ┆   ┆        ┆        ┆        ┆              │\n",
      "│ XS2376095068 ┆ INVESCO DIG.  ┆ 2022-01-28 ┆ 32.988 ┆ … ┆ 32.766 ┆ 32.988 ┆ 0      ┆ 2.34         │\n",
      "│              ┆ ETC21 BITC.   ┆            ┆        ┆   ┆        ┆        ┆        ┆              │\n",
      "│ XS2376095068 ┆ INVESCO DIG.  ┆ 2022-01-31 ┆ 33.26  ┆ … ┆ 33.126 ┆ 33.26  ┆ 0      ┆ 0.82         │\n",
      "│              ┆ ETC21 BITC.   ┆            ┆        ┆   ┆        ┆        ┆        ┆              │\n",
      "└──────────────┴───────────────┴────────────┴────────┴───┴────────┴────────┴────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    s3_objects_keys: list[str] = list_s3_objects(s3_client, start_date, end_date, bucket_src_data)\n",
    "    s3_file_objects: list[io.StringIO] = fetch_multiple_s3_objects(s3_client, s3_objects_keys, bucket_src_data)\n",
    "    df_raw: pl.DataFrame = create_polars_df(s3_file_objects)\n",
    "    df_transformed: pl.DataFrame = transform_df(df_raw)\n",
    "    key_of_uploaded_s3_object: str = write_to_s3(df_transformed)\n",
    "    df_from_uploaded_s3_object: pl.DataFrame = fetch_single_s3_object(s3_client, bucket_tgt_data, key_of_uploaded_s3_object)\n",
    "    print(df_from_uploaded_s3_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
